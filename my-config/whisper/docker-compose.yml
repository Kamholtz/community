version: '3.8'

services:
  # Faster-whisper backend - lighter, faster setup
  faster-whisper:
    build:
      context: .
      dockerfile: Dockerfile.faster-whisper
    image: whisper-dictate:faster-whisper
    container_name: whisper-faster
    environment:
      - DISPLAY=${DISPLAY}
      - PULSE_SERVER=unix:${XDG_RUNTIME_DIR}/pulse/native
    volumes:
      # X11 forwarding for clipboard access
      - /tmp/.X11-unix:/tmp/.X11-unix:rw
      # PulseAudio socket
      - ${XDG_RUNTIME_DIR}/pulse:${XDG_RUNTIME_DIR}/pulse:rw
      # Model cache (optional - speeds up subsequent runs)
      - whisper-models:/home/whisper/.cache/huggingface
    devices:
      # Audio device access
      - /dev/snd:/dev/snd
    network_mode: host
    stdin_open: true
    tty: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    profiles: ["faster-whisper"]

  # WhisperX backend - best accuracy, word alignment
  whisperx:
    build:
      context: .
      dockerfile: Dockerfile.whisperx
    image: whisper-dictate:whisperx  
    container_name: whisper-x
    environment:
      - DISPLAY=${DISPLAY}
      - PULSE_SERVER=unix:${XDG_RUNTIME_DIR}/pulse/native
    volumes:
      # X11 forwarding for clipboard access
      - /tmp/.X11-unix:/tmp/.X11-unix:rw
      # PulseAudio socket
      - ${XDG_RUNTIME_DIR}/pulse:${XDG_RUNTIME_DIR}/pulse:rw
      # Model cache (optional - speeds up subsequent runs)
      - whisper-models:/home/whisper/.cache/huggingface
    devices:
      # Audio device access
      - /dev/snd:/dev/snd
    network_mode: host
    stdin_open: true
    tty: true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    profiles: ["whisperx"]

  # HTTP API service (external faster-whisper server)
  whisper-api:
    image: fedirz/faster-whisper-server:latest
    container_name: whisper-api
    ports:
      - "8000:8000"
    environment:
      - MODEL=Systran/faster-distil-whisper-small.en
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    profiles: ["api"]

volumes:
  whisper-models:
    driver: local